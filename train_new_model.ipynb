{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install sklearn #bei Nutzung mit Kaggle relevant\n",
    "#pip install flair==0.6.1 #bei Nutzung mit Kaggle relevant\n",
    "#!conda install -y gdown #bei Nutzung mit Kaggle relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import json\n",
    "import flair, torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standart Code von Kaggle.com\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "#import os\n",
    "\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#    for filename in filenames:\n",
    "#        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flair.device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Data/train_data_ctxt_preliminary.json\"\n",
    "\n",
    "with open(filename,'r', encoding='utf8') as infile:\n",
    "    example_data = json.load(infile)\n",
    "\n",
    "\n",
    "for i,(k,v) in enumerate(example_data.items()):\n",
    "    tokens = v.get('tokens')\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    example_data[k]['tokens'] = tokens\n",
    "    \n",
    "all_tokens = [v['tokens'] for k,v in example_data.items()]\n",
    "all_labels = [v['labels'] for k,v in example_data.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234567890)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_tokens, all_labels, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4421 4421\n",
      "1474 1474\n",
      "1474 1474\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(y_train))\n",
    "print(len(X_test), len(y_test))\n",
    "print(len(X_val), len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file does not exist\n",
      "The file does not exist\n",
      "The file does not exist\n",
      "The file does not exist\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists(\"TrainTestVal/all.txt\"):\n",
    "  os.remove(\"all.txt\")\n",
    "else:\n",
    "  print(\"The file does not exist\")\n",
    "\n",
    "f = open(\"TrainTestVal/all.txt\", \"x\")\n",
    "\n",
    "for x in range(0, len(all_tokens)):\n",
    "    for y in range(0, len(all_tokens[x])):\n",
    "        new_line = all_tokens[x][y] + \" \" + all_labels[x][y] + \"\\n\"\n",
    "        f.write(new_line)\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "f.close()\n",
    "\n",
    "if os.path.exists(\"TrainTestVal/train.txt\"):\n",
    "  os.remove(\"TrainTestVal/train.txt\")\n",
    "else:\n",
    "  print(\"The file does not exist\")\n",
    "\n",
    "f = open(\"TrainTestVal/train.txt\", \"x\")\n",
    "\n",
    "for x in range(0, len(X_train)):\n",
    "    for y in range(0, len(X_train[x])):\n",
    "        new_line = X_train[x][y] + \" \" + y_train[x][y] + \"\\n\"\n",
    "        f.write(new_line)\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "if os.path.exists(\"TrainTestVal/test.txt\"):\n",
    "  os.remove(\"TrainTestVal/test.txt\")\n",
    "else:\n",
    "  print(\"The file does not exist\")\n",
    "\n",
    "f = open(\"TrainTestVal/test.txt\", \"x\")\n",
    "\n",
    "for x in range(0, len(X_test)):\n",
    "    for y in range(0, len(X_test[x])):\n",
    "        new_line = X_test[x][y] + \" \" + y_test[x][y] + \"\\n\"\n",
    "        f.write(new_line)\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "f.close()\n",
    "\n",
    "if os.path.exists(\"TrainTestVal/val.txt\"):\n",
    "  os.remove(\"TrainTestVal/val.txt\")\n",
    "else:\n",
    "  print(\"The file does not exist\")\n",
    "\n",
    "f = open(\"TrainTestVal/val.txt\", \"x\")\n",
    "\n",
    "for x in range(0, len(X_val)):\n",
    "    for y in range(0, len(X_val[x])):\n",
    "        new_line = X_val[x][y] + \" \" + y_val[x][y] + \"\\n\"\n",
    "        f.write(new_line)\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-22 11:50:48,375 Reading data from .\n",
      "2020-10-22 11:50:48,377 Train: TrainTestVal/train.txt\n",
      "2020-10-22 11:50:48,378 Dev: TrainTestVal/val.txt\n",
      "2020-10-22 11:50:48,379 Test: TrainTestVal/test.txt\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus\n",
    "\n",
    "# define columns\n",
    "columns = {0: 'text', 1: 'ner'}\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "data_folder = './'\n",
    "\n",
    "# init a corpus using column format, data folder and the names of the train, dev and test files\n",
    "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
    "                              train_file='TrainTestVal/train.txt',\n",
    "                              test_file='TrainTestVal/test.txt',\n",
    "                              dev_file='TrainTestVal/val.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary with 20 tags: <unk>, O, B_C, I_C, B_E, B_T, B_P, I_P, B_M, I_M, B_V, I_V, I_T, B_W, I_E, B_A, I_A, I_W, <START>, <STOP>\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, TransformerWordEmbeddings, FlairEmbeddings, CharacterEmbeddings, OneHotEmbeddings\n",
    "tag_type = 'ner'\n",
    "\n",
    "# 3. make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
    "print(tag_dictionary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-22 11:50:54,659 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-22 11:50:54,660 Model: \"SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): WordEmbeddings('glove')\n",
      "    (list_embedding_1): FlairEmbeddings(\n",
      "      (lm): LanguageModel(\n",
      "        (drop): Dropout(p=0.05, inplace=False)\n",
      "        (encoder): Embedding(300, 100)\n",
      "        (rnn): LSTM(100, 2048)\n",
      "        (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (list_embedding_2): FlairEmbeddings(\n",
      "      (lm): LanguageModel(\n",
      "        (drop): Dropout(p=0.05, inplace=False)\n",
      "        (encoder): Embedding(300, 100)\n",
      "        (rnn): LSTM(100, 2048)\n",
      "        (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=4196, out_features=4196, bias=True)\n",
      "  (rnn): LSTM(4196, 128, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (linear): Linear(in_features=256, out_features=20, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2020-10-22 11:50:54,664 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-22 11:50:54,665 Corpus: \"Corpus: 4421 train + 1474 dev + 1474 test sentences\"\n",
      "2020-10-22 11:50:54,667 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-22 11:50:54,678 Parameters:\n",
      "2020-10-22 11:50:54,680  - learning_rate: \"0.1\"\n",
      "2020-10-22 11:50:54,684  - mini_batch_size: \"32\"\n",
      "2020-10-22 11:50:54,692  - patience: \"3\"\n",
      "2020-10-22 11:50:54,696  - anneal_factor: \"0.5\"\n",
      "2020-10-22 11:50:54,700  - max_epochs: \"150\"\n",
      "2020-10-22 11:50:54,704  - shuffle: \"True\"\n",
      "2020-10-22 11:50:54,706  - train_with_dev: \"False\"\n",
      "2020-10-22 11:50:54,708  - batch_growth_annealing: \"False\"\n",
      "2020-10-22 11:50:54,710 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-22 11:50:54,712 Model training base path: \"resources/taggers/NewModel\"\n",
      "2020-10-22 11:50:54,714 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-22 11:50:54,716 Device: cpu\n",
      "2020-10-22 11:50:54,720 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-22 11:50:54,723 Embeddings storage mode: gpu\n",
      "2020-10-22 11:50:54,727 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-22 11:55:07,846 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-22 11:55:07,849 Exiting from training early.\n",
      "2020-10-22 11:55:07,850 Saving model ...\n",
      "2020-10-22 11:55:18,436 Done.\n",
      "2020-10-22 11:55:18,444 ----------------------------------------------------------------------------------------------------\n",
      "2020-10-22 11:55:18,459 Testing using best model ...\n"
     ]
    }
   ],
   "source": [
    "#Hier können die Hyperparameter konfiguriert werden\n",
    "#ein oder mehrere Embeddings, Language Models einfach auskommentieren\n",
    "#Hidden size der LSTM Layer kann definiert werden\n",
    "#Layer Anzahl kann definiert werden\n",
    "#CRF Layer\n",
    "\n",
    "embedding_types = [\n",
    "\n",
    "    WordEmbeddings('glove'),\n",
    "    # TransformerWordEmbeddings(\"bert-base-uncased\"),\n",
    "\n",
    "    #CharacterEmbeddings(),\n",
    "\n",
    "    # comment in these lines to use flair embeddings\n",
    "    FlairEmbeddings('news-forward'),\n",
    "    FlairEmbeddings('news-backward')\n",
    "    #ELMoEmbeddings('medium')\n",
    "]\n",
    "\n",
    "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "\n",
    "\n",
    "# 5. initialize sequence tagger\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "tagger: SequenceTagger = SequenceTagger(hidden_size=128, #128, 256, 512\n",
    "                                        embeddings=embeddings,\n",
    "                                        tag_dictionary=tag_dictionary,\n",
    "                                        #loss_weights = weight_dict,\n",
    "                                        rnn_layers = 2, #Number of rnn_layers\n",
    "                                        tag_type=tag_type,\n",
    "                                        use_crf=True)\n",
    "\n",
    "# 6. initialize trainer\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
    "    \n",
    "    \n",
    "    \n",
    "# 7. start training\n",
    "trainer.train('resources/taggers/NewModel',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              max_epochs=150,\n",
    "             embeddings_storage_mode = \"gpu\")#oder cpu\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "\n",
    "# load the model you trained\n",
    "model = SequenceTagger.load('resources/taggers/NewModel/best-model.pt')\n",
    "\n",
    "\n",
    "# create example sentence\n",
    "sentence = Sentence('friendly people were here for a birthday Party')\n",
    "\n",
    "# predict tags and print\n",
    "model.predict(sentence)\n",
    "\n",
    "print(sentence.to_tagged_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
